name: CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      cleanup:
        description: 'Clean up infrastructure'
        required: true
        type: boolean
        default: false

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install backend dependencies
        run: |
          cd app/backend
          pip install -e .
          pip install pytest pytest-asyncio httpx
          
      - name: Run backend tests
        run: |
          cd app/backend
          pytest
          
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          
      - name: Install frontend dependencies
        run: |
          cd app/frontend
          npm install
          
      - name: Run frontend tests
        run: |
          cd app/frontend
          npm test
          
  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
          
      - name: Build and push backend
        uses: docker/build-push-action@v5
        with:
          context: ./app/backend
          push: true
          tags: ${{ secrets.DOCKERHUB_USERNAME }}/k8s-url-shortener-backend:latest
          
      - name: Build and push frontend
        uses: docker/build-push-action@v5
        with:
          context: ./app/frontend
          push: true
          tags: ${{ secrets.DOCKERHUB_USERNAME }}/k8s-url-shortener-frontend:latest

  infrastructure:
    needs: build
    if: github.event.inputs.cleanup != 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.7"

      - name: Create terraform.tfvars from template
        run: |
          cp terraform/terraform.tfvars.template terraform/terraform.tfvars

      - name: Terraform Init
        working-directory: ./terraform
        run: |
          terraform init \
            -backend-config="bucket=${{ vars.TERRAFORM_STATE_BUCKET || 'k8s-url-shortener-terraform-state' }}" \
            -backend-config="key=${{ vars.TERRAFORM_STATE_KEY || 'terraform.tfstate' }}" \
            -backend-config="region=${{ vars.AWS_REGION || 'us-east-1' }}" \
            -backend-config="dynamodb_table=${{ vars.TERRAFORM_LOCK_TABLE || 'k8s-url-shortener-terraform-locks' }}" \
            -backend-config="encrypt=${{ vars.TERRAFORM_STATE_ENCRYPT || 'true' }}"

      - name: Terraform Plan
        working-directory: ./terraform
        run: |
          terraform plan \
            -var="region=${{ vars.AWS_REGION || 'us-east-1' }}" \
            -var="cluster_name=${{ vars.CLUSTER_NAME || 'k8s-url-shortener' }}" \
            -var="instance_type=${{ vars.INSTANCE_TYPE || 't2.micro' }}" \
            -var="terraform_state_bucket=${{ vars.TERRAFORM_STATE_BUCKET || 'k8s-url-shortener-terraform-state' }}" \
            -var="terraform_state_key=${{ vars.TERRAFORM_STATE_KEY || 'terraform.tfstate' }}" \
            -var="terraform_lock_table=${{ vars.TERRAFORM_LOCK_TABLE || 'k8s-url-shortener-terraform-locks' }}" \
            -var="terraform_state_encrypt=${{ vars.TERRAFORM_STATE_ENCRYPT || 'true' }}" \
            -out=tfplan

      - name: Terraform Apply
        working-directory: ./terraform
        run: terraform apply -auto-approve tfplan

      - name: Get kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ vars.CLUSTER_NAME || 'k8s-url-shortener' }} --region ${{ vars.AWS_REGION || 'us-east-1' }}
          kubectl config view --raw > kubeconfig.yaml
          echo "KUBECONFIG_BASE64=$(cat kubeconfig.yaml | base64 -w 0)" >> $GITHUB_ENV
          echo "CLUSTER_NAME=${{ vars.CLUSTER_NAME || 'k8s-url-shortener' }}" >> $GITHUB_ENV
          echo "AWS_REGION=${{ vars.AWS_REGION || 'us-east-1' }}" >> $GITHUB_ENV

  deploy:
    needs: infrastructure
    if: github.event.inputs.cleanup != 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure kubectl
        run: |
          # First try to use the kubeconfig from the infrastructure job
          if [ -n "${{ env.KUBECONFIG_BASE64 }}" ]; then
            echo "${{ env.KUBECONFIG_BASE64 }}" | base64 -d > kubeconfig.yaml
            export KUBECONFIG=kubeconfig.yaml
          else
            # Fallback to generating new kubeconfig
            aws eks update-kubeconfig --name ${{ vars.CLUSTER_NAME || 'k8s-url-shortener' }} --region ${{ vars.AWS_REGION || 'us-east-1' }}
          fi

      - name: Verify cluster access
        run: |
          if ! kubectl cluster-info; then
            echo "Failed to access cluster"
            exit 1
          fi

      - name: Deploy to EKS
        run: |
          # Create Docker Hub secret
          kubectl create secret docker-registry dockerhub-secret \
            --docker-server=https://index.docker.io/v1/ \
            --docker-username=${{ secrets.DOCKERHUB_USERNAME }} \
            --docker-password=${{ secrets.DOCKERHUB_TOKEN }} \
            --docker-email=${{ secrets.DOCKERHUB_EMAIL || 'not-set' }} \
            -n default || true

          # Patch default service account to use the secret
          kubectl patch serviceaccount default -n default \
            -p '{"imagePullSecrets": [{"name": "dockerhub-secret"}]}'

          # Apply Kubernetes manifests
          if ! kubectl kustomize k8s/base/ | kubectl apply -f -; then
            echo "Failed to apply Kubernetes manifests"
            exit 1
          fi

          # Wait for deployments to be ready
          echo "Waiting for deployments to be ready..."
          
          # Check pod status and events
          echo "Checking pod status..."
          kubectl get pods -n default
          echo "Checking pod events..."
          kubectl get events -n default --sort-by='.lastTimestamp'
          
          # Wait for deployments with increased timeout
          echo "Waiting for backend deployment..."
          if ! kubectl wait --for=condition=available --timeout=600s deployment/url-shortener-backend -n default; then
            echo "Backend deployment failed to become available"
            echo "Checking backend pod logs..."
            kubectl logs -l app=url-shortener,component=backend -n default
            echo "Checking backend pod events..."
            kubectl describe pod -l app=url-shortener,component=backend -n default
            exit 1
          fi
          
          echo "Waiting for frontend deployment..."
          if ! kubectl wait --for=condition=available --timeout=600s deployment/url-shortener-frontend -n default; then
            echo "Frontend deployment failed to become available"
            echo "Checking frontend pod logs..."
            kubectl logs -l app=url-shortener,component=frontend -n default
            echo "Checking frontend pod events..."
            kubectl describe pod -l app=url-shortener,component=frontend -n default
            exit 1
          fi

  cleanup:
    if: github.event.inputs.cleanup == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.7"

      - name: Create terraform.tfvars from template
        run: |
          cp terraform/terraform.tfvars.template terraform/terraform.tfvars

      - name: Terraform Init
        working-directory: ./terraform
        run: |
          terraform init \
            -backend-config="bucket=${{ vars.TERRAFORM_STATE_BUCKET || 'k8s-url-shortener-terraform-state' }}" \
            -backend-config="key=${{ vars.TERRAFORM_STATE_KEY || 'terraform.tfstate' }}" \
            -backend-config="region=${{ vars.AWS_REGION || 'us-east-1' }}" \
            -backend-config="dynamodb_table=${{ vars.TERRAFORM_LOCK_TABLE || 'k8s-url-shortener-terraform-locks' }}" \
            -backend-config="encrypt=${{ vars.TERRAFORM_STATE_ENCRYPT || 'true' }}"

      - name: Terraform Destroy
        working-directory: ./terraform
        run: |
          # First try to destroy the EKS cluster
          if ! terraform destroy -auto-approve -target=module.eks; then
            echo "Failed to destroy EKS cluster, continuing with full destroy..."
          fi
          
          # Then destroy everything else
          terraform destroy -auto-approve 